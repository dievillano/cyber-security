# Methods {#sec-methods}

## The UNSW-NB15 Network Dataset

The UNSW-NB15 dataset [@moustafa2015unsw; @moustafa2016evaluation] was developed to address the limitations of earlier benchmarks such as KDD99 and NSL-KDD, which have been criticised for outdated attack types, unrealistic normal traffic, and inconsistent training–test distributions. UNSW-NB15 combines modern real-world network activity with synthetically generated attacks, offering 49 features from both flow-level interactions and deep packet inspection. It includes nine contemporary attack categories and updated normal traffic profiles, making it well-suited for evaluating Network Intrusion Detection Systems (NIDSs).

The dataset contains 2,540,044 records, with 87% representing normal traffic, reflecting real-world class imbalance [@zoghi2021unsw]. Training (175,341 records) and testing (82,332 records) subsets were obtained from the [UNSW website](https://research.unsw.edu.au/projects/unsw-nb15-dataset) [@unsw2015nb15]. Both subsets share similar non-linear and non-normal feature distributions, as well as high statistical correlation, supporting their use in benchmarking. After excluding non-informative variables, 42 predictors remained alongside two targets: `label` (binary attack indicator) and `attack_cat` (attack category), described in @tbl-dict.

To better reflect realistic network conditions, we created a more representative imbalanced subset by retaining only normal and denial-of-service (DoS) traffic. This yielded 82.03% normal and 17.97% DoS records, closely mirroring the distribution in the full dataset.

## Feature Engineering

Exploratory data analysis was performed to assess feature quality and distributions. The first anomaly involved `is_ftp_login` and `ct_ftp_cmd`, which were identical in the training set, with integer values from 0 to 4. As `is_ftp_login` is defined as binary, this suggested possible corruption. The values matched the definition of `ct_ftp_cmd`, so `is_ftp_login` was dropped. This issue did not occur in the test set, where the two features differed as expected.

The nominal variables `proto`, `service`, and `state` had 133, 13, and 9 unique values, respectively, but most records were concentrated in a few categories. To reduce dimensionality and aid interpretability, infrequent levels were grouped into an “other” category using the Pareto principle, retaining those covering roughly 90% of records. This reduced `proto` to 5 levels, `service` to 4, and `state` to 4.

Many numeric features were highly right-skewed with a peak at zero, likely indicating unsuccessful or dropped connections. Several showed discrete value patterns. For example, `sttl` and `dttl` often appeared near 0, 30, 60, and 250, consistent with known initial time-to-live (TTL) values of 64, 128, and 255, plus an additional cluster near 30 [@ttl]. These were recoded as ordinal variables with levels “0”, “\~30”, “\~64”, and “\~255”. Similarly, `swin` and `dwin` were usually 0 or 255, with other values occurring rarely, so they were discretised into “0”, “255”, and “rare”.

Before feature selection and modelling, nominal features were one-hot encoded, ordinal features mapped to integers, and numeric features log-transformed and scaled using a robust scaler. This approach, based on the median and interquartile range, is more appropriate for skewed data than standard scaling. For numeric variables with zeros, we tested log transformations with an offset of 1, an offset equal to half the smallest non-zero value [@hyndman2013transformations], and the Yeo–Johnson transformation. Visual inspection showed that the latter offset method best improved normality, so it was applied to zero-containing features, while strictly positive features used the standard log transformation.


## Feature Selection

To reduce redundancy and retain informative predictors, we used a two-stage feature selection strategy combining correlation analysis and model-based permutation importance.

First, after preprocessing (see earlier section), we computed the Spearman correlation matrix for numeric features in the training set. Pairs with correlation ≥ 0.9 were flagged, and for each pair, the feature with the higher mutual information (MI) score relative to the binary target was retained. MI, estimated via a nearest-neighbour method [@ross2014mutual; @sklearn-mi], captures any statistical dependency between variables and is particularly suitable for discrete–continuous relationships. This step removed redundant features while preserving those most relevant for classification.

Second, we assessed feature relevance using a random forest (RF) classifier and permutation importance. The training data was split into training and validation subsets (80/20) via stratified sampling to maintain class balance. RF hyperparameters were tuned using a grid search with 5-fold stratified cross-validation, optimising the F1 score. We varied the number of trees ({1000, 1500}), maximum depth ({3, 5}), and minimum samples per split ({10, 15}), and applied class weighting using the “balanced” scheme [@sklearn-rfclassifier].

Permutation importance was computed on the validation set as the mean decrease in F1 after shuffling each feature over 10 repetitions. Importance scores for one-hot encoded variables were aggregated by their original category (e.g., all `proto_*` columns under `proto`). We then selected the top five base features and retained all associated encoded columns, producing a compact, interpretable predictor set for subsequent modelling.

## Bayesian Neural Networks (BNNs)

### Neural Networks (NNs)

Neural networks (NNs) are hierarchical models composed of an input layer, one or more hidden layers, and an output layer, where each layer consists of units that perform a linear transformation followed by a non-linear activation function [@arbel2023primer]. Training a neural network involves finding the set of weights and biases at the hidden and output layers that minimise a specified loss function, typically using gradient-based optimisation algorithms, such as stochastic gradient descent (SGD), and backpropagation [@jospin2022hands].

Formally, given an input vector $\mathbf{x} \in \mathbb{R}^n$, a neural network with $L$ hidden layers of widths $H_1, \dots, H_L$, and a non-linear activation function $\phi: \mathbb{R} \rightarrow \mathbb{R}$, the computations at layer $l$ ($l = 1, \dots, L$) are:

$$
\begin{aligned}
\mathbf{g}^{(l)}(\mathbf{x}) &= \mathbf{w}^{(l)} \mathbf{h}^{(l-1)}(\mathbf{x}) + \mathbf{b}^{(l)} \\
\mathbf{h}^{(l)}(\mathbf{x}) &= \phi\left(\mathbf{g}^{(l)}(\mathbf{x})\right),
\end{aligned}
$$

where $\mathbf{w}^{(l)}$ is the weight matrix of dimensions $H_l \times H_{l-1}$, $\mathbf{b}^{(l)}$ is a bias vector of length $H_l$, $\mathbf{h}^{(l-1)}(\mathbf{x})$ denotes the post-activation values of the previous layer (with $\mathbf{h}^{(0)} = \mathbf{x}$), and $\mathbf{g}^{(l)}(\mathbf{x})$ are the pre-activation values. For the output layer, the pre-activation value is $g^{(L+1)}(\mathbf{x}) = \mathbf{w}^{(L+1)} \mathbf{h}^{(L)}(\mathbf{x}) + b^{(L+1)}$, where, in the binary classification setting, $\mathbf{w}^{(L+1)} \in \mathbb{R}^{H_L}$ is the weight vector connecting the last hidden layer to the single output neuron, and $b^{(L+1)} \in \mathbb{R}$ is the scalar bias. The output-layer activation function is selected to match the target variable’s distribution; for example, the sigmoid function is used for a Bernoulli-distributed binary outcome $y \in \{0,1\}$. While $\phi$ is often fixed across layers, it may vary depending on the network architecture or specific application [@arbel2023primer]. Some popular choices are the hyperbolic tangent function (tanh), the rectified linear unit (ReLU) function, and the softmax function for multi-class classification tasks.

A widely used loss function for binary classification is the binary cross-entropy (BCE), also known as log loss. Let $\theta$ denote the set of all parameters (weights and biases) in the neural network, and let $f(\mathbf{x}_i; \theta)$ represent the predicted probability output by the network for input $\mathbf{x}_i$. Given a dataset $\{(\mathbf{x}_i, y_i)\}_{i=1}^N$, where $y_i \in \{0,1\}$, the BCE is defined as

$$
J(\theta) = -\sum_{i=1}^N \big[ y_i \log f(\mathbf{x}_i; \theta) + (1-y_i) \log \big( 1 - f(\mathbf{x}_i; \theta) \big) \big].
$$ {#eq-logloss}

Training a NN for binary classification therefore amounts to finding the set of parameters $\hat{\theta}$ that minimises this loss:

$$
\hat{\theta} = \underset{\mathbf{w}}{\mathrm{arg\,min}} \; J(\mathbf{w}).
$$

Training seeks $\hat{\theta} = \arg\min{\theta} J(\theta)$, often using SGD or the Adam [@adam2014method] optimiser. Adam extends SGD by adapting learning rates and incorporating momentum, improving convergence in practice. This approach corresponds to maximum likelihood estimation under a frequentist framework [@goodfellow2016deep]. NNs perform strongly across diverse tasks due to their expressive power, architectural inductive biases, and flexibility in applying regularisation [@arbel2023primer].

### The Bayesian Approach

Despite its practical succes, the frequentist approach to neural networks presents important limitations, particularly in safety-critical real-world applications such as medical diagnosis and cyber-security [@arbel2023primer]. In particular, neural networks often produce miscalibrated or overconfident predicted class probabilities in classification tasks, are sensitive to out-of-distribution samples and domain shifts, are vulnerable to adversarial attacks, lack inherent human interpretability (often functioning as “black-box” models), and may generalise poorly when data is limited [@arbel2023primer; @jospin2022hands]. A variety of strategies have been proposed to address these issues, with Bayesian neural networks (BNNs) standing out as one of the most rigorous and conceptually intuitive frameworks for building robust, uncertainty-aware models [@jospin2022hands].

BNNs differ from their frequentist counterparts in that the parameters $\theta \in \Theta$ are treated as random variables endowed with a prior distribution $p(\theta)$. Given a dataset $D = \{(\mathbf{x}_i, y_i)\}_{i=1}^N$ and a likelihood function $p(D | \theta)$ describing how the parameters generate the observed data, the Bayesian approach seeks to infer the posterior distribution

$$
p(\theta | D) = \frac{p(D | \theta)\, p(\theta)}{p(D)} \propto p(D | \theta)\, p(\theta).
$$

For NNs, however, this posterior is typically a high-dimensional, highly non-convex probability distribution [@jospin2022hands]. Moreover, computing it exactly requires evaluating the evidence

$$
p(D) = \int_{\Theta} p(D | \theta)\, p(\theta) \, d\theta,
$$

an integral over a vast and non-linear parameter space. This calculation is analytically intractable and computationally prohibitive, even for moderately sized networks. Consequently, to estimate the posterior, BNNs rely on sampling methods such as Markov chain Monte Carlo (MCMC) or approximation methods such as variational inference (VI) [@jospin2022hands].

Given $p(\theta | D)$, the posterior predictive distribution for a new observation $y^{*}$ associated with some input $\mathbf{x}^{*}$, $p(y^{*} | \mathbf{x}^{*}, D)$, is given by

$$
p(y^{*} | \mathbf{x}^{*}, D) = \text{E}\!\left[p(y^{*} | \mathbf{x}^{*}, \theta) | D\right] 
= \int_{\Theta} p(y^{*} | \mathbf{x}^{*}, \theta) \, p(\theta | D) \, d\theta.
$$

Direct evaluation is typically intractable, so it is approximated via Monte Carlo by drawing samples $\theta^{(s)} \sim p(\theta | D)$, computing $p(y^{*} | \mathbf{x}^{*}, \theta^{(s)})$, and averaging [@jospin2022hands]. This integrates over parameter uncertainty (epistemic uncertainty), which differs from aleatoric uncertainty, arising from irreducible data noise [@jospin2022hands; @arbel2023primer].

BNNs quantify uncertainty by modelling the posterior over network parameters, producing better-calibrated predictions than conventional NNs and expressing high epistemic uncertainty for out-of-distribution inputs [@jospin2022hands]. They also allow prior knowledge to be encoded as inductive bias, acting as soft constraints akin to regularisation. Common deep learning methods such as ensembling or data augmentation can be interpreted in this Bayesian framework, which both deepens theoretical insight and guides the design of new strategies when exact inference is infeasible [@jospin2022hands].

### Priors

Priors in Bayesian Neural Networks (BNNs) encode beliefs about parameter values before observing data and can influence accuracy, log-likelihood, and probability calibration [@izmailov2021bayesian]. However, specifying priors for high-dimensional, over-parameterised models is difficult due to the limited interpretability of parameters and the intractability of the true posterior [@arbel2023primer; @wenzel2020good].

Regularisation methods in frequentist NNs, such as L2 and L1 penalties, correspond to Gaussian and Laplace priors respectively [@jospin2022hands]. A common default in BNNs is a zero-mean Gaussian prior with diagonal covariance, though high-variance, weakly informative Gaussians often perform better and are robust across wide variance ranges [@murphy2012machine; @izmailov2021bayesian]. Heavy-tailed priors, such as logistic, can also yield slight gains [@izmailov2021bayesian].

BNNs trained with variational inference and stochastic gradient descent often induce heavy-tailed parameter distributions, diverging from Gaussian assumptions [@arbel2023primer]. Some argue Gaussian priors may be unsuitable [@wenzel2020good], but network architecture typically has a greater impact on performance than the specific prior [@izmailov2021bayesian], though prior sensitivity checks remain good practice.

### Inference Methods

#### Markov Chain Monte Carlo (MCMC)

Markov Chain Monte Carlo (MCMC) methods approximate the posterior distribution of BNN parameters by constructing a Markov chain whose stationary distribution matches the desired posterior [@jospin2022hands]. While conceptually straightforward, applying MCMC to BNNs is computationally challenging due to the high dimensionality of the parameter space and the strong correlations between parameters[@jospin2022hands]. While generic MCMC algorithms like Gibbs sampling are ill-suited to BNNs, the Metropolis–Hastings (MH) algorithm is more applicable, as it requires to evaluate only a function proportional to the posterior, effectively avoiding the evaluation of its normalising constant [@jospin2022hands]. 

The MH algorithm requires the user to chose a proposal distribution $Q(\theta^{*}|\theta)$ from which candidate samples $\theta^{*}$ for the target posterior distribution are drawn. Given a target posterior distribution $p(\theta|D) \propto p(D|\theta)p(\theta)$ and denoting $f(\theta)=p(D|\theta)p(\theta)$, the MH algorithm generates a set of samples (i.e., a Markov chain) $\{\theta^{(t)}\}_{t=1}^T$ as follows:

1. Set initial values $\theta^{(0)}$
2. For $t = 0, \dots, T-1$ do:
    - Given the current state $\theta^{(t)}=\theta$, draw a candidate $\theta^{*} \sim Q(\theta^{*} | \theta)$.
    - Compute the acceptance probability:
    $$
    \gamma = \min\left(1, \frac{f(\theta^{*})Q(\theta|\theta^{*})}{f(\theta)Q(\theta^{*}|\theta)}\right).
    $$
    - Draw $u \sim \mathcal{U}(0,1)$.
    - If $u \leq \gamma$, accept the candidate and set $\theta^{(t+1)}=\theta^{*}$, else set $\theta^{(t+1)}=\theta$.

Once convergence is achieved, these posterior samples can be used to approximate expectations of functions $h(\theta)$ under $p(\theta|D)$ via the Monte Carlo estimate

$$
\text{E}[h(\theta)|D] \approx \frac{1}{T} \sum_{t=1}^T h(\theta^{(t)}).
$$

In practice, the MH algorithm is run with a warm-up (burn-in) phase to allow convergence, and these initial samples are discarded. The remaining draws approximate the posterior, with quality depending on the mixing rate—how well the chain explores the parameter space—which in high-dimensional BNNs can be hindered by poor proposals or strong parameter correlations. Mixing can be assessed visually (trace or autocorrelation plots) or numerically, with the potential scale reduction statistic $\hat{R}$ and the effective sample size (ESS) being common diagnostics [@stan_ref]. Values of $\hat{R}$ close to 1 indicate convergence, while a low ESS signals high autocorrelation and poor mixing.

Hamiltonian Monte Carlo (HMC) [@neal2011mcmc] improves MH efficiency by using gradient information to generate proposals along Hamiltonian trajectories, reducing random-walk behaviour and improving mixing in high dimensions [@hoffman2014no]. Its performance depends on the trajectory length (longer reduces autocorrelation but increases cost) and step size (smaller improves accuracy but requires more gradients), both of which require tuning. The No-U-Turn Sampler (NUTS) [@hoffman2014no] removes the need to set these parameters by adaptively stopping trajectories when they double back and using primal–dual averaging to tune step size, making it a robust, largely automated choice for sampling from the complex posteriors of BNNs.

#### Variational Inference (VI) {#sec-vi}

While MCMC methods provide exact samples from the posterior, their limited scalability has shifted focus towards variational inference (VI) as a more computationally efficient alternative for Bayesian neural networks (BNNs). VI approximates the true posterior $p(\theta | D)$ with a variational distribution $q_{\phi}(\theta)$, parameterized by $\phi$ (i.e., parameters such as location, scale, etc.). This variational distribution is optimised to minimize the Kullback-Leibler (KL) divergence between the two distributions, which is a measure of distance between the distributions [@jospin2022hands]. Denoting $q_{\phi}=q$, the KL divergence is given by:

$$
\begin{aligned}
\text{KL}\left(q \mid \mid p(\theta, D)\right)&=\int_{\Theta}{q(\theta)\text{log}\left(\frac{q(\theta)}{p(\theta|D)}\right)d\theta} \\
&=-\int_{\Theta}{q(\theta)\text{log}\left(\frac{p(\theta)p(D|\theta)}{q(\theta)}\right)d\theta}+\text{log}p(D)
\end{aligned}
$$

Given that the $\text{log}p(D)$ does not depend on the choice of $q$, minimising the KL divergence is equivalent to minimising the first term, which is called the evidence lower bound (ELBO):

$$
\begin{aligned}
\text{ELBO}(q)&=\int_{\Theta}{q(\theta)\text{log}\left(\frac{p(\theta)p(D|\theta)}{q(\theta)}\right)d\theta} \\
&=\int_{\Theta}{q(\theta)\text{log}p(D|\theta)d\theta}-\text{KL}\left(q \mid \mid p(\theta)\right) \\
&=\text{E}\left(\text{log}p(D|\theta)\right)-\text{KL}\left(q \mid \mid p(\theta)\right)
\end{aligned}
$$

The first component represents the expected likelihood, promoting a distribution $q$ that effectively account for the observed data. The second component corresponds to the negative KL divergence between $q$ and the prior, encouraging the variational distribution to remain close to the prior. Together, these terms reflect the balance between likelihood and prior [@blei2017variational]. The ELBO can be efficiently optimized using stochastic gradient descent (SGD) or Adam optimizers, enabling scalability to large datasets [@jospin2022hands]. Distributions from the exponential family, particularly Gaussian distributions, are popular choices for the variational distribution $q$ due to their mathematical convenience and tractability [@jospin2022hands].

Several widely used VI algorithms for BNNs, including Bayes-by-Backprop [@blundell2015weight] and probabilistic backpropagation [@hernandez2015probabilistic], rely on the mean-field assumption, which treats network parameters $\theta_k$ as mutually independent each governed by distinct factor $q_{k}(\theta_k)$ in the variational distribution [@blei2017variational]:

$$
q(\theta)=\prod_{m=1}^{M}{q_{k}(\theta_k)},
$$

where $M$ is the total number of parameters in the neural network. Although this assumption simplifies computation and facilitates scalable inference, it is often overly restrictive and can lead to underestimated uncertainty by ignoring dependencies among parameters [@arbel2023primer]. More expressive variational distributions, such as full-covariance multivariate Gaussians, aim to mitigate these limitations. Nonetheless, VI methods are known to suffer from mode collapse, focusing on a single mode of the posterior despite the multimodal nature commonly observed in BNN posteriors [@arbel2023primer]. Consequently, achieving accurate variational approximations in deep neural networks remains challenging and typically requires careful hyperparameter tuning [@arbel2023primer].

## Benchmarking 

We evaluated both the predictive performance and the calibration quality of the predicted attack probabilities of BNN classifiers with varying configurations for detecting DoS cyber-attacks using the UNSW-NB15 dataset. Predictive performance was assessed using precision, recall, and the $F_1$ score. Precision quantifies the proportion of correctly identified attacks among all instances predicted as attacks, while recall measures the proportion of correctly identified attacks among all actual attacks. The $F_1$ score, defined as the harmonic mean of precision and recall, provides a single metric that balances both aspects. All three metrics take values in $[0,1]$, with values closer to 1 indicating superior performance. THRESHOLD

Calibration quality was evaluated using the log loss as defined in $\ref{eq-logloss}$, the Brier score, and the expected calibration error (ECE). Lower log loss values indicate better match between the predicted probabilities and the true labels. The Brier score (BS) for binary classification is given by

$$
\text{BS}=\frac{1}{N}\sum_{i=1}^{N}{(y_i-\hat{p}_i)^2},
$$

where $y_i \in \{0,1\}$ is the true class label for observation $i$ and $\hat{p}_i$ is the predicted probability of success, $\Pr(y_i=1)$. Lower BS values indicate better prediction probabilities. The ECE measures the average absolute difference between predicted probabilities and observed accuracies across $M$ equally spaced probability bins, and is defined as

$$
\text{ECE} = \sum_{m=1}^M \frac{|B_m|}{N} \, \big| \text{acc}(B_m) - \text{conf}(B_m) \big|,
$$

where $B_m$ denotes the set of predictions whose predicted probabilities fall within the $m$-th bin, $\text{acc}(B_m)$ is the average accuracy within bin $B_m$, and $\text{conf}(B_m)$ is the average predicted probability within bin $B_m$ [@guo2017calibration]. Lower values of ECE indicate better alignment between predicted probabilities and empirical outcomes. In this study, we set $M=10$. Calibration quality of the final classifiers was also assessed using calibration curves, which plot empirical accuracy against mean predicted confidence within discrete probability bins (as in the ECE computation). Curves that lie close to the identity line indicate well-calibrated predictions.

BNNs classifiers with a single hidden layer ($L = 1$) and ReLU activations were trained under multiple configurations and evaluated on a validation set of 1,000 samples randomly drawn from the training subset. BNNs were implemented in NumPyro [@phan2019composable; @bingham2019pyro], a probabilistic programming library for Python. In NumPyro, variational distributions are referred to as guides, and we adopt this terminology here. The BNN configurations differed in:

1. Hidden layer width ($H_1$): 5, 10, and 14 (equal to the number of input features).
2. Prior mean precision: Prior means of 0.01, 0.1, and 1.
3. Inference method: Markov Chain Monte Carlo (MCMC) using the No-U-Turn Sampler (NUTS) vs. Variational Inference (VI).
4. Guide type (VI only): mean-field Gaussian and multivariate Gaussian.

The hidden layer widths were chosen to reflect different model complexities, approximately corresponding to one-third, two-thirds, and the full input dimension. A zero-mean Gaussian prior was placed on all network parameters $\theta$, assumed independent and identically distributed (i.i.d.). The prior variances followed a formulation analogous to the He initialisation scheme for neural networks [@he2015delving], which is particularly effective for ReLU activations, scaling parameter variances by the width of the preceding layer. Specifically, for the hidden layer weights and biases:

$$
w_{ij}^{(1)} \sim \mathcal{N}\left(0, \frac{\sigma^2}{14}\right) \quad \text{and} \quad b_{i}^{(1)} \sim \mathcal{N}\left(0, \frac{\sigma^2}{14}\right),
$$

for $i = 1, \ldots, 14$ (input dimension) and $j = 1, \ldots, H_1$ ($H_1 \in \{5, 10, 14\}$), and for the output layer pre-activations:

$$
w_{j}^{(2)} \sim \mathcal{N}\left(0, \frac{\sigma^2}{H_1}\right) \quad \text{and} \quad b^{(2)} \sim \mathcal{N}\left(0, \frac{\sigma^2}{H_1}\right),
$$

for $j = 1, \ldots, H_1$. The precision parameter $\tau=1/\sigma^2$ was assigned a Gamma prior, $\tau \sim \mathcal{G}(\alpha, \beta)$. To explore the influence of this prior, we varied the prior mean while keeping the coefficient of variation constant: 0.01 ($\alpha=2,\ \beta=200$), 0.1 ($\alpha=2,\ \beta=20$), and 1 ($\alpha=2,\ \beta=2$). This variation allowed us to assess (i) whether the prior meaningfully influences posterior inference (i.e., whether the posterior shifts with different prior means), and (ii) whether higher or lower prior variance affects BNN performance, as some previous work has suggested that higher-variance priors may lead to improved predictive accuracy.

Denote the training dataset as $\{(\mathbf{x}_i, y_i)\}_{i=1}^{N}$, where $N=67{,}264$. The outputs $y_i$ ($i=1,\ldots,N$) for the BNNs classifiers were assumed Bernoulli-distributed with probability of attack $p_i$ and a sigmoid activation through a logit function:

$$
\begin{aligned}
y_i &= \mathcal{B}(p_i), \\
\text{logit}(p_i) &= \text{log}\left(\frac{p_i}{1-p_i}\right) \\
&= \mathbf{w}^{(2)}\mathbf{h}^{(1)}(\mathbf{x}_i) + b^{(2)},
\end{aligned}
$$

where $\mathbf{x}_i$ is the input vector for sample $i$ and $\mathbf{h}^{(1)}(\mathbf{x}_i)$ is the hidden layer output. 

For MCMC, the NUTS algorithm was run with two chains, each generating 500 post-warm-up samples after 250 warm-up iterations. Parameters were initialised at their prior means, and all other settings used NumPyro defaults (target acceptance probability $\gamma=0.8$). Summary statistics and convergence diagnostics were computed for each parameter, including posterior mean, standard deviation, median, highest posterior density interval (HPDI), ESS, and the $\hat{R}$ statistic.

For VI, both mean-field and multivariate Gaussian guides were created using NumPyro’s automatic guide generation. Guides were initialised with means randomly chosen from a uniform distribution with support $[-2, 2]$ and standard deviations of 0.1 (default options). The multivariate Gaussian guide was initialised as a diagonal Gaussian (no correlations), but unlike the mean-field guide, it could learn correlations during optimisation. The Adam optimiser was used with a defaul learning rate of $10^{-3}$. ELBO trace plots were monitored to verify convergence, with convergence indicated by a plateau in the ELBO curve. 

BNN model selection was based solely on the validation $F_1$ score. The selected BNN was compared against (i) a frequentist neural network with identical architecture but without parameter priors, (ii) a gradient-boosted decision tree (GBDT), which is a strong baseline for classification tasks, particularly on tabular data, and (iii) calibrated versions of both classifiers.

The frequentist NN was trained using mini-batches of size 256 for 20 epochs, optimised with Adam (learning rate $10^{-3}$). Early stopping was applied with a patience of 5 epochs based on the validation $F_1$ score. The GBDT classifier was trained with 1,000 trees, a maximum depth of 3, a subsampling rate of 0.8 for individual trees, and a minimum of 10 samples required to split an internal node. Calibrated versions of both classifiers were constructed as follows: the training set was randomly split into five stratified folds; for each split, the classifiers were fitted to the training subset (using the same hyperparameters as the un-calibrated classifiers), and their outputs were calibrated on the corresponding validation subset using isotonic regression [@scikit-learn-calibratedclass]. Isotonic regression is a non-parametric method that fits a stepwise, non-decreasing function $\hat{f}$ by minimising

$$
\sum_{i=1}^N (y_i - \hat{f}_i)^2,
$$

where $y_i \in \{0,1\}$ is the true label for observation $i$, and $\hat{f}_i$ is the calibrated probability of success [@scikit-learn-calibration]. Final predicted probabilities were obtained by averaging the outputs of the individually calibrated classifiers across the five folds [@scikit-learn-calibratedclass].



