% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  a4paper,
]{scrreprt}
\usepackage{xcolor}
\usepackage[top=2.5cm, bottom=2.5cm, left=3cm, right=3cm]{geometry}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}



\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\usepackage[font=small, labelfont={bf,small}, format=plain]{caption}
\usepackage{caption}
\captionsetup[longtable]{position=bottom}
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{suppfig}{h}{losuppfig}}{\newfloat{suppfig}{h}{losuppfig}[chapter]}
\floatname{suppfig}{Figure S}
\newcommand*\quartosuppfigref[1]{Figure \hyperref[#1]{S\ref{#1}}}
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\DeclareCaptionLabelFormat{quartosuppfigreflabelformat}{#1#2}
\captionsetup[suppfig]{labelformat=quartosuppfigreflabelformat}
\newcommand*\listofsuppfigs{\listof{suppfig}{List of Supplementary Figuress}}
\makeatother
\makeatletter
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{plain}
\@ifundefined{c@chapter}{\newfloat{supptbl}{h}{losupptbl}}{\newfloat{supptbl}{h}{losupptbl}[chapter]}
\floatname{supptbl}{Table S}
\newcommand*\quartosupptblref[1]{Table \hyperref[#1]{S\ref{#1}}}
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\DeclareCaptionLabelFormat{quartosupptblreflabelformat}{#1#2}
\captionsetup[supptbl]{labelformat=quartosupptblreflabelformat}
\newcommand*\listofsupptbls{\listof{supptbl}{List of Supplementary Tabless}}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Bayesian Neural Networks},
  pdfauthor={Diego Cesar Villa Almeyda},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Bayesian Neural Networks}
\author{Diego Cesar Villa Almeyda}
\date{August, 2025}
\begin{document}
\cleardoublepage
\thispagestyle{empty}
\vspace*{2cm}  % Adds top space, adjust as needed

\begin{center}
  {\huge\bfseries Bayesian Neural Networks \par}
    
  \vspace{6em}

    {\Large\bfseries Diego Cesar Villa Almeyda \par}
  
  \vspace{2em}
  {\bfseries\large Master of Science \par}
  
  \vspace{1.5em}
  {\bfseries\large August, 2025 \par}
  
  \vspace{4em}

      {\bfseries\large School of Mathematics \par}
    
  {\bfseries\large The University of Edinburgh \par}
      
  \vspace{6em}
  {\small Dissertation Presented for the Degree of MSc in Statistics with Data Science \par}
\end{center}

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

This is a Quarto book.

To learn more about Quarto books visit
\url{https://quarto.org/docs/books}.

\bookmarksetup{startatroot}

\chapter*{Executive summary}\label{executive-summary}
\addcontentsline{toc}{chapter}{Executive summary}

\markboth{Executive summary}{Executive summary}

\bookmarksetup{startatroot}

\chapter{Introduction}\label{introduction}

This is a book created from markdown and executable code.

\bookmarksetup{startatroot}

\chapter{Methods}\label{methods}

\section{The UNSW-NB15 Network
Dataset}\label{the-unsw-nb15-network-dataset}

The UNSW-NB15 dataset (1,2) was created to overcome the limitations of
earlier benchmark datasets such as KDD99 and NSL-KDD, which have been
criticised for outdated attack types, unrealistic normal traffic, and
inconsistent distributions between training and testing sets. In
contrast, UNSW-NB15 combines modern real-world network activity with
synthetically generated attack behaviours, making it highly suitable for
evaluating contemporary Network Intrusion Detection Systems (NIDSs). The
dataset contains 49 features encompassing both flow-level host
interactions and deep packet inspection metrics, enabling effective
discrimination between normal and malicious traffic. It includes nine
categories of contemporary cyberattacks alongside updated profiles of
normal network behaviour. Statistically, UNSW-NB15 is more complex than
its predecessors (2).

The full dataset comprises 2,540,044 records, of which 2,218,761
(approximately 87\%) correspond to normal traffic, resulting in a highly
imbalanced class distribution that reflects real-world network
conditions (3). The training and testing subsets were obtained directly
from the
\href{https://research.unsw.edu.au/projects/unsw-nb15-dataset}{UNSW
website} (4), consisting of 175,341 and 82,332 records, respectively.
Statistical analysis has demonstrated that the training and test sets
share similar non-linear and non-normal feature distributions.
Furthermore, high statistical correlation between the two sets supports
their appropriateness as benchmark data for evaluating statistical and
machine learning models tasked with distinguishing complex attack
patterns from normal traffic (2). Non-informative features were excluded
from the distributed datasets, yielding a total of 42 usable predictors
and two target variables: \texttt{label} (binary attack indicator) and
\texttt{attack\_cat} (attack category), as described in
\quartosupptblref{supptbl-dict}.

An initial examination of the training dataset revealed that it contains
a disproportionate number of attack records (68.06\%) compared to normal
traffic (31.93\%), which does not reflect realistic conditions. To
create a more representative imbalanced subset for our analysis, we
retained only the normal traffic and denial-of-service (DoS) attack
instances. This resulted in a subset with 82.03\% normal and 17.97\% DoS
traffic, closely aligning with the class distribution in the full
dataset.

\section{Feature Engineering}\label{feature-engineering}

Exploratory data analysis was conducted to assess the quality and
distribution of the features. One of the first issues identified was
with the features \texttt{is\_ftp\_login} and \texttt{ct\_ftp\_cmd},
which were found to be identical in the training dataset and contained
integer values ranging from 0 to 4. This was unexpected, as
\texttt{is\_ftp\_login} is defined as a binary variable in the data
dictionary, suggesting possible data corruption. Given that the observed
values appeared to align with the definition of \texttt{ct\_ftp\_cmd},
we chose to drop \texttt{is\_ftp\_login} from the training set.
Interestingly, this anomaly was not observed in the test set, where the
two features differed as expected.

The nominal features \texttt{proto}, \texttt{service}, and
\texttt{state} had 133, 13, and 9 unique levels, respectively. However,
only a small subset of these levels accounted for the vast majority of
records in the training data. To reduce dimensionality and improve model
interpretability, we grouped the infrequent levels in each feature into
a single ``other'' category. We applied a Pareto principle approach,
retaining the levels that together covered approximately 90\% of the
records. After grouping, \texttt{proto} was reduced to 5 levels,
\texttt{service} to 4, and \texttt{state} to 4, resulting in a more
manageable set of categories for downstream modelling.

Most numeric features exhibited strong right-skewness, with a pronounced
peak at zero, likely indicating unsuccessful or dropped connections.
Further inspection revealed that several variables take values from a
limited set of predefined ranges. For example, \texttt{sttl} and
\texttt{dttl}, which represent source and destination time-to-live (TTL)
values, frequently appeared near 0, 30, 60, and 250. According to (5),
typical initial TTL values are 64, 128, and 255, which gradually
decrease during transmission---consistent with the observed values,
along with the additional cluster near 30. As these values reflect
discrete categories rather than continuous magnitudes, we recoded them
as ordinal variables with levels ``0'', ``\textasciitilde30'',
``\textasciitilde64'', and ``\textasciitilde255''. Similarly, the
features \texttt{swin} and \texttt{dwin} predominantly took values of 0
and 255, with other values appearing very infrequently (often only
once). This pattern suggested that 0 and 255 may also be predefined
values. Accordingly, we discretised these numeric features into the
nominal levels ``0'', ``255'', and ``rare''.

Before feature selection and model building, nominal features were
one-hot encoded, ordinal features were mapped to integers, and numeric
features were log-transformed and scaled using a robust scaler. The
robust scaler subtracts the median and divides by the interquartile
range (IQR), making it more suitable for highly skewed data than
standard scaling, which assumes a roughly symmetric distribution.
Because most numeric features were heavily skewed and included zero
values, we explored various log-based transformations. For features with
zeros, we tested log transformations with two offsets: adding 1, and
adding half the smallest non-zero value, as suggested by (6). We also
tested the Yeo--Johnson transformation. Visual inspection showed that
the latter offset approach provided the best normalisation, so we
applied it to features containing zeros, while using the standard log
transformation for strictly positive features.

\section{Feature Selection}\label{feature-selection}

To reduce redundancy and retain the most informative features, we
applied a two-stage feature selection strategy combining correlation
analysis and model-based permutation importance.

First, after preprocessing (see earlier section), we computed the
Spearman correlation matrix for the numeric features in the training
set. Feature pairs with a high correlation (≥ 0.9) were flagged, and for
each pair, we retained the feature with the higher mutual information
(MI) score relative to the binary target variable. MI measures the
amount of shared information between two variables, capturing any form
of statistical dependency---beyond linear correlation---and is
particularly well-suited for assessing relationships between a discrete
and a continuous variable (7). MI was estimated using a
nearest-neighbour-based method, which avoids the resolution loss
associated with binning and provides a more accurate, non-parametric
measure of association (8). This filtering step helped reduce redundancy
while preserving features most informative for the classification task.

In the second stage, we employed a random forest (RF) classifier to
assess feature relevance using permutation importance. The training data
was split into a sub-training and validation set using stratified
sampling to preserve the class distribution, allocating 20\% of the data
for validation. A model was then trained using a grid search with 5-fold
stratified cross-validation, optimising the F1 score. Although this was
not the final predictive model, we carefully selected the hyperparameter
grid to reduce overfitting. Specifically, we tuned the number of trees
(\{1000, 1500\}), maximum tree depth (\{3, 5\}), and minimum number of
samples required to split an internal node (\{10, 15\}) (9). In
addition, we addressed class imbalance by applying class weights in the
learning algorithm, using the ``balanced'' scheme, which assigns weights
inversely proportional to class frequencies (9). After selecting the
best-performing model, permutation importance was computed on the
validation set by measuring the average decrease in F1 score when each
feature was randomly shuffled across 10 repetitions. To further simplify
the feature space, we grouped the importance scores of one-hot encoded
variables by their original categorical variable (e.g., all
\texttt{proto\_*} columns were aggregated under \texttt{proto}). We then
selected the top ten base features and retained all corresponding
encoded columns, yielding a compact and interpretable set of predictors
for subsequent modelling.

\section{Bayesian Neural Networks
(BNN)}\label{bayesian-neural-networks-bnn}

Neural networks (NNs) are hierarchical models composed of an input
layer, one or more hidden layers, and an output layer, where each layer
consists of units that perform a linear transformation followed by a
non-linear activation function (10). Training a neural network involves
finding the set of weights and biases at the hidden and output layers
that minimise a specified loss function, typically using gradient-based
optimisation algorithms, such as stochastic gradient descent (SGD), and
backpropagation (11).

Formally, given an input vector \(\mathbf{x} \in \mathbb{R}^n\), a
neural network with \(L\) hidden layers of widths \(H_1, \dots, H_L\),
and a non-linear activation function
\(\phi: \mathbb{R} \rightarrow \mathbb{R}\), the computations at layer
\(l\) (\(l = 1, \dots, L\)) are:

\[
\begin{aligned}
\mathbf{g}^{(l)}(\mathbf{x}) &= \mathbf{w}^{(l)} \mathbf{h}^{(l-1)}(\mathbf{x}) + \mathbf{b}^{(l)} \\
\mathbf{h}^{(l)}(\mathbf{x}) &= \phi\left(\mathbf{g}^{(l)}(\mathbf{x})\right),
\end{aligned}
\]

where \(\mathbf{w}^{(l)}\) is the weight matrix of dimensions
\(H_l \times H_{l-1}\), \(\mathbf{b}^{(l)}\) is a bias vector of length
\(H_l\), \(\mathbf{h}^{(l-1)}(\mathbf{x})\) denotes the post-activation
values of the previous layer (with \(\mathbf{h}^{(0)} = \mathbf{x}\)),
and \(\mathbf{g}^{(l)}(\mathbf{x})\) are the pre-activation values. For
the output layer, the pre-activation values are computed as
\(\mathbf{g}^{(L+1)}(\mathbf{x}) = \mathbf{w}^{(L+1)} \mathbf{h}^{(L)}(\mathbf{x}) + \mathbf{b}^{(L+1)}\),
and the activation function for the output layer is chosen to match the
distribution of the target variable; for example, a sigmoid function for
a Bernoulli-distributed binary outcome \(y \in {0,1}\). While \(\phi\)
is often fixed across layers, it may vary depending on the network
architecture or specific application (10).

A widely used loss function for binary classification is the binary
cross-entropy (BCE), also known as log loss. Let \(\mathbf{w}\) denote
the set of all parameters (weights and biases) in the neural network,
and let \(f(\mathbf{x}_i; \mathbf{w})\) represent the predicted
probability output by the network for input \(\mathbf{x}_i\). Given a
dataset \(\{(\mathbf{x}_i, y_i)\}_{i=1}^N\), where \(y_i \in \{0,1\}\),
the BCE is defined as

\[
J(\mathbf{w}) = -\sum_{i=1}^N \big[ y_i \log f(\mathbf{x}_i; \mathbf{w}) + (1-y_i) \log \big( 1 - f(\mathbf{x}_i; \mathbf{w}) \big) \big].
\]

Training a NN for binary classification therefore amounts to finding the
parameter set \(\mathbf{w}\) that minimises this loss:

\[
\hat{\mathbf{w}} = \underset{\mathbf{w}}{\mathrm{arg\,min}} \; J(\mathbf{w}).
\]

This optimisation is typically performed using SGD, where gradients are
estimated at each iteration using randomly selected subsets of the data,
known as mini-batches (10). Because the optimiser does not need to
process the entire dataset at each step, but only a small random portion
of it, SGD is particularly well suited for large-scale datasets. Another
widely used stochastic optimiser is Adam (adaptive moment estimation),
which extends SGD by incorporating adaptive learning rates and momentum
terms. Adam is computationally efficient, requires minimal memory, and
often converges faster in practice (12).

\subsection{Priors}\label{priors}

\subsection{Inference Methods}\label{inference-methods}

\subsubsection{Markov Chain Monte Carlo
(MCMC)}\label{markov-chain-monte-carlo-mcmc}

\subsubsection{Variational Inference
(VI)}\label{variational-inference-vi}

\subsection{Convergence}\label{convergence}

\section{Model Benchmarking}\label{model-benchmarking}

\subsection{Prediction Accuracy}\label{prediction-accuracy}

\subsection{Calibration}\label{calibration}

\subsection{Running Time}\label{running-time}

\section{Interpretability Analysis}\label{interpretability-analysis}

\bookmarksetup{startatroot}

\chapter{Results}\label{results}

\bookmarksetup{startatroot}

\chapter{Conclusions}\label{conclusions}

\bookmarksetup{startatroot}

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{0}{1}
\bibitem[\citeproctext]{ref-moustafa2015unsw}
\CSLLeftMargin{1. }%
\CSLRightInline{Moustafa N, Slay J. UNSW-NB15: A comprehensive data set
for network intrusion detection systems (UNSW-NB15 network data set).
In: 2015 military communications and information systems conference
(MilCIS). IEEE; 2015. p. 1--6. }

\bibitem[\citeproctext]{ref-moustafa2016evaluation}
\CSLLeftMargin{2. }%
\CSLRightInline{Moustafa N, Slay J. The evaluation of network anomaly
detection systems: Statistical analysis of the UNSW-NB15 data set and
the comparison with the KDD99 data set. Information Security Journal: A
Global Perspective. 2016;25(1-3):18--31. }

\bibitem[\citeproctext]{ref-zoghi2021unsw}
\CSLLeftMargin{3. }%
\CSLRightInline{Zoghi Z, Serpen G. UNSW-NB15 computer security dataset:
Analysis through visualization. arXiv preprint arXiv:210105067. 2021; }

\bibitem[\citeproctext]{ref-unsw2015nb15}
\CSLLeftMargin{4. }%
\CSLRightInline{University of New South Wales (UNSW). UNSW-NB15 dataset
{[}Internet{]}. 2015. Available from:
\url{https://research.unsw.edu.au/projects/unsw-nb15-dataset}}

\bibitem[\citeproctext]{ref-ttl}
\CSLLeftMargin{5. }%
\CSLRightInline{Imperva. Time to live (TTL) {[}Internet{]}. n.d.
Available from:
\url{https://www.imperva.com/learn/performance/time-to-live-ttl/}}

\bibitem[\citeproctext]{ref-hyndman2013transformations}
\CSLLeftMargin{6. }%
\CSLRightInline{Hyndman RJ. Transforming data with zeros {[}Internet{]}.
2013. Available from:
\url{https://robjhyndman.com/hyndsight/transformations/}}

\bibitem[\citeproctext]{ref-ross2014mutual}
\CSLLeftMargin{7. }%
\CSLRightInline{Ross BC. Mutual information between discrete and
continuous data sets. PloS one. 2014;9(2):e87357. }

\bibitem[\citeproctext]{ref-sklearn-mi}
\CSLLeftMargin{8. }%
\CSLRightInline{scikit‑learn Developers. Scikit\-learn:
mutual\_info\_classif.
\url{https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.mutual_info_classif.html};
2025. }

\bibitem[\citeproctext]{ref-sklearn-rfclassifier}
\CSLLeftMargin{9. }%
\CSLRightInline{scikit‑learn Developers. Scikit\-learn:
RandomForestClassifier.
\url{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html};
2025. }

\bibitem[\citeproctext]{ref-arbel2023primer}
\CSLLeftMargin{10. }%
\CSLRightInline{Arbel J, Pitas K, Vladimirova M, Fortuin V. A primer on
bayesian neural networks: Review and debates. arXiv preprint
arXiv:230916314. 2023; }

\bibitem[\citeproctext]{ref-jospin2022hands}
\CSLLeftMargin{11. }%
\CSLRightInline{Jospin LV, Laga H, Boussaid F, Buntine W, Bennamoun M.
Hands-on bayesian neural networks---a tutorial for deep learning users.
IEEE Computational Intelligence Magazine. 2022;17(2):29--48. }

\bibitem[\citeproctext]{ref-adam2014method}
\CSLLeftMargin{12. }%
\CSLRightInline{Adam KDBJ et al. A method for stochastic optimization.
arXiv preprint arXiv:14126980. 2014;1412(6). }

\end{CSLReferences}

\bookmarksetup{startatroot}

\chapter*{Appendix}\label{appendix}
\addcontentsline{toc}{chapter}{Appendix}

\markboth{Appendix}{Appendix}

\section*{Supplementary Tables}\label{supplementary-tables}
\addcontentsline{toc}{section}{Supplementary Tables}

\markright{Supplementary Tables}

\begin{supptbl}

\centering{

\scriptsize

\begin{longtable*}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.1275}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.0604}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.8121}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Description
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
id & Integer & Record ID. \\
dur & Float & Record total duration. \\
proto & Nominal & Transaction protocol. \\
service & Nominal & Such as http, ftp, smtp, ssh, dns and ftp-data. \\
state & Nominal & Indicates to the state and its dependent protocol
(such as ACC, CLO and CON). \\
spkts & Integer & Source to destination packet count. \\
dpkts & Integer & Destination to source packet count. \\
sbytes & Integer & Source to destination transaction bytes. \\
dbytes & Integer & Destination to source transaction bytes. \\
rate & Float & Ethernet data rates transmitted and received. \\
sttl & Integer & Source to destination time to live value. \\
dttl & Integer & Destination to source time to live value. \\
sload & Float & Source bits per second. \\
dload & Float & Destination bits per second. \\
sloss & Integer & Source packets retransmitted or dropped. \\
dloss & Integer & Destination packets retransmitted or dropped. \\
sinpkt & Float & Source interpacket arrival time (mSec). \\
dinpkt & Float & Destination interpacket arrival time (mSec). \\
sjit & Float & Source jitter (mSec). \\
djit & Float & Destination jitter (mSec). \\
swin & Integer & Source TCP window advertisement value. \\
stcpb & Integer & Source TCP base sequence number. \\
dtcpb & Integer & Destination TCP base sequence number. \\
dwin & Integer & Destination TCP window advertisement value. \\
tcprtt & Float & TCP connection setup round-trip time, the sum of synack
and ackdat. \\
synack & Float & TCP connection setup time, the time between the SYN and
the SYN\_ACK packets. \\
ackdat & Float & TCP connection setup time, the time between the
SYN\_ACK and the ACK packets. \\
smean & Integer & Mean of the flow packet size transmitted by the
src. \\
dmean & Integer & Mean of the flow packet size transmitted by the
dst. \\
trans\_depth & Integer & Represents the pipelined depth into the
connection of http request/response transaction. \\
response\_body\_len & Integer & Actual uncompressed content size of the
data transferred from the server's http service. \\
ct\_srv\_src & Integer & No.~of connections that contain the same
service and source address in 100 connections according to the last
time. \\
ct\_state\_ttl & Integer & No.~for each state according to specific
range of values for source/destination time-to-live. \\
ct\_dst\_ltm & Integer & No.~of connections of the same destination
address in 100 connections according to the last time. \\
ct\_src\_dport\_ltm & Integer & No of connections of the same source
address and the destination port in 100 connections according to the
last time. \\
ct\_dst\_sport\_ltm & Integer & No of connections of the same
destination address and the source port in 100 connections according to
the last time. \\
ct\_dst\_src\_ltm & Integer & No of connections of the same source and
the destination address in in 100 connections according to the last
time. \\
is\_ftp\_login & Binary & If the ftp session is accessed by user and
password then 1 else 0. \\
ct\_ftp\_cmd & Integer & No of flows that has a command in ftp
session. \\
ct\_flw\_http\_mthd & Integer & No.~of flows that has methods such as
Get and Post in http service. \\
ct\_src\_ltm & Integer & No.~of records of the srcip in 100 records
according to the ltime. \\
ct\_srv\_dst & Integer & No.~of connections that contain the same
service and destination address in 100 connections according to the last
time. \\
is\_sm\_ips\_ports & Binary & If source and destination IP addresses
equal and port numbers equal then, this variable takes value 1 else
0. \\
attack\_cat & Nominal & The name of each attack category. \\
label & Binary & Normal (0) or attack (1) label. \\
\end{longtable*}

}

\caption{\label{supptbl-dict}}

\end{supptbl}%




\end{document}
